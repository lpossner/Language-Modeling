{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a39fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:57:43.527196Z",
     "iopub.status.busy": "2025-02-06T19:57:43.526543Z",
     "iopub.status.idle": "2025-02-06T19:57:46.617589Z",
     "shell.execute_reply": "2025-02-06T19:57:46.616745Z"
    },
    "papermill": {
     "duration": 3.097307,
     "end_time": "2025-02-06T19:57:46.619718",
     "exception": false,
     "start_time": "2025-02-06T19:57:43.522411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84218db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:57:46.626141Z",
     "iopub.status.busy": "2025-02-06T19:57:46.625282Z",
     "iopub.status.idle": "2025-02-06T19:57:46.693908Z",
     "shell.execute_reply": "2025-02-06T19:57:46.692975Z"
    },
    "papermill": {
     "duration": 0.073483,
     "end_time": "2025-02-06T19:57:46.695712",
     "exception": false,
     "start_time": "2025-02-06T19:57:46.622229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 1000    # Vocabulary size\n",
    "embed_dim = 512      # Embedding dimension\n",
    "num_heads = 8        # Number of attention heads\n",
    "ff_hidden_dim = 2048 # Feedforward hidden dimension\n",
    "num_layers = 6       # Number of transformer decoder layers\n",
    "max_seq_len = 128    # Maximum sequence length\n",
    "num_epochs = 10      # Number of training epochs\n",
    "learning_rate = 3e-4 # Learning rate (increased for from-scratch training)\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "input_directory = kagglehub.dataset_download(\"mruanova/shakespeare\")\n",
    "input_filepath = os.path.join(input_directory, \"shakespeare.txt\")\n",
    "output_filepath = os.path.join(\"results\", \"shakespeare-transformer\")\n",
    "\n",
    "Path(output_filepath).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c908f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:57:46.701607Z",
     "iopub.status.busy": "2025-02-06T19:57:46.701052Z",
     "iopub.status.idle": "2025-02-06T19:57:46.716238Z",
     "shell.execute_reply": "2025-02-06T19:57:46.715251Z"
    },
    "papermill": {
     "duration": 0.020507,
     "end_time": "2025-02-06T19:57:46.718352",
     "exception": false,
     "start_time": "2025-02-06T19:57:46.697845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = self.head_dim ** -0.5  # scaling factor for query\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Causal mask (upper triangular, prevents attending to future tokens)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(x.device)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))\n",
    "        attn_scores = attn_scores + causal_mask\n",
    "\n",
    "        # Attention probabilities\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attn_output = torch.matmul(attn_probs, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Final output projection\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.self_attn = CausalSelfAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-LN: LayerNorm before self-attention + residual connection\n",
    "        attn_output = self.self_attn(self.layernorm1(x))\n",
    "        x = x + self.dropout(attn_output)\n",
    "\n",
    "        # Pre-LN: LayerNorm before feedforward + residual connection\n",
    "        ffn_output = self.ffn(self.layernorm2(x))\n",
    "        x = x + self.dropout(ffn_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, max_seq_len, dropout=0.1):\n",
    "        super(CausalTransformerDecoder, self).__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "        for _ in range(num_layers)])\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)  # Final layer norm\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Token + position embeddings\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.embed_tokens(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Final layer norm\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        # Final linear projection to vocab size\n",
    "        logits = self.output_proj(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa738d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:57:46.727189Z",
     "iopub.status.busy": "2025-02-06T19:57:46.726864Z",
     "iopub.status.idle": "2025-02-06T19:57:49.166129Z",
     "shell.execute_reply": "2025-02-06T19:57:49.165454Z"
    },
    "papermill": {
     "duration": 2.445802,
     "end_time": "2025-02-06T19:57:49.168092",
     "exception": false,
     "start_time": "2025-02-06T19:57:46.722290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, input_filepath, output_filepath, seq_len):\n",
    "        self.tokens = []\n",
    "        with open(input_filepath, \"r\") as file:\n",
    "            corpus = \"\".join(file.readlines())\n",
    "        model_prefix = os.path.join(output_filepath, \"sentencepiece\")\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=input_filepath,\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=model_prefix + \".model\")\n",
    "        tokens = self.sp.EncodeAsIds(corpus)\n",
    "        for idx in range(0, len(tokens) - seq_len):\n",
    "            self.tokens.append(tokens[idx:idx + seq_len + 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.tokens[idx]\n",
    "        input_ids = sequence[:-1]\n",
    "        labels = sequence[1:]\n",
    "        return torch.tensor(input_ids), torch.tensor(labels)\n",
    "\n",
    "dataset = TextDataset(input_filepath=input_filepath, output_filepath=output_filepath, seq_len=max_seq_len)\n",
    "# Use num_workers=0 for Jupyter notebooks to avoid multiprocessing issues\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb53cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:57:49.174084Z",
     "iopub.status.busy": "2025-02-06T19:57:49.173800Z",
     "iopub.status.idle": "2025-02-06T19:57:50.779347Z",
     "shell.execute_reply": "2025-02-06T19:57:50.778773Z"
    },
    "papermill": {
     "duration": 1.610687,
     "end_time": "2025-02-06T19:57:50.781327",
     "exception": false,
     "start_time": "2025-02-06T19:57:49.170640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CausalTransformerDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_seq_len=max_seq_len,\n",
    ")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.sp.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9679ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:57:50.787096Z",
     "iopub.status.busy": "2025-02-06T19:57:50.786741Z",
     "iopub.status.idle": "2025-02-06T22:55:44.884567Z",
     "shell.execute_reply": "2025-02-06T22:55:44.883204Z"
    },
    "papermill": {
     "duration": 10674.103015,
     "end_time": "2025-02-06T22:55:44.886704",
     "exception": false,
     "start_time": "2025-02-06T19:57:50.783689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (input_ids, labels) in enumerate(dataloader):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # Reshape logits and labels for the loss function\n",
    "        logits = logits.view(-1, vocab_size)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "        labels = labels.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Progress [{batch_idx/len(dataloader)*100:.4f}%], Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # Average loss for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54ca65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:55:45.653248Z",
     "iopub.status.busy": "2025-02-06T22:55:45.652898Z",
     "iopub.status.idle": "2025-02-06T22:55:45.833153Z",
     "shell.execute_reply": "2025-02-06T22:55:45.832469Z"
    },
    "papermill": {
     "duration": 0.558132,
     "end_time": "2025-02-06T22:55:45.835135",
     "exception": false,
     "start_time": "2025-02-06T22:55:45.277003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_output_path = os.path.join(output_filepath, 'model_weights.pth')\n",
    "torch.save(model.state_dict(), model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, sp, prompt, max_length=100, temperature=1.0, top_k=50):\n",
    "    \"\"\"\n",
    "    Generate text from the model given a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CausalTransformerDecoder\n",
    "        sp: SentencePiece processor\n",
    "        prompt: Input text string to start generation\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: Number of top tokens to sample from (0 = no filtering)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt\n",
    "    input_ids = sp.EncodeAsIds(prompt)\n",
    "    input_ids = torch.tensor([input_ids]).to(device)\n",
    "    \n",
    "    generated_ids = input_ids[0].tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Limit context to max_seq_len\n",
    "            context = input_ids[:, -max_seq_len:]\n",
    "            \n",
    "            # Get logits\n",
    "            logits = model(context)\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits = torch.full_like(logits, float('-inf'))\n",
    "                logits[top_k_indices] = top_k_values\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated_ids.append(next_token.item())\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop if we generate EOS token\n",
    "            if next_token.item() == sp.eos_id():\n",
    "                break\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = sp.DecodeIds(generated_ids)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test text generation\n",
    "prompt = \"To be or not to be\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGenerated text:\")\n",
    "generated = generate_text(model, dataset.sp, prompt, max_length=100, temperature=0.8, top_k=50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2513d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5837480,
     "sourceId": 9575633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10686.838146,
   "end_time": "2025-02-06T22:55:47.877540",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-06T19:57:41.039394",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
